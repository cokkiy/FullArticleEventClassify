{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"./data/test.json\")\n",
    "\n",
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns= ['text', 'id'])\n",
    "chunked_dataset['train'][:10]\n",
    "dataset\n",
    "chunked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arguments_dataset2 import ArgumentsDataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from commonfn import align_labels, get_token_index\n",
    "\n",
    "label_to_index = {\n",
    "    # convert label to index\n",
    "    'O': 0,\n",
    "    'B-Subject': 1,\n",
    "    'I-Subject': 2,\n",
    "    'B-Equipment': 3,\n",
    "    'I-Equipment': 4,\n",
    "    'B-Date': 5,\n",
    "    'I-Date': 6,\n",
    "    'B-Location': 7,\n",
    "    'I-Location': 8,\n",
    "    'B-Area': 9,\n",
    "    'I-Area': 10,\n",
    "    'B-Content': 11,\n",
    "    'I-Content': 12,\n",
    "    'B-Militaryforce': 13,\n",
    "    'I-Militaryforce': 14,\n",
    "    'B-Object': 15,\n",
    "    'I-Object': 16,\n",
    "    'B-Materials': 17,\n",
    "    'I-Materials': 18,\n",
    "    'B-Result': 19,\n",
    "    'I-Result': 20,\n",
    "    'B-Quantity': 21,\n",
    "    'I-Quantity': 22\n",
    "}\n",
    "\n",
    "label_names = {\n",
    "    0: 'O',\n",
    "    1: 'B-Subject',\n",
    "    2: 'I-Subject',\n",
    "    3: 'B-Equipment',\n",
    "    4: 'I-Equipment',\n",
    "    5: 'B-Date',\n",
    "    6: 'I-Date',\n",
    "    7: 'B-Location',\n",
    "    8: 'I-Location',\n",
    "    9: 'B-Area',\n",
    "    10: 'I-Area',\n",
    "    11: 'B-Content',\n",
    "    12: 'I-Content',\n",
    "    13: 'B-Militaryforce',\n",
    "    14: 'I-Militaryforce',\n",
    "    15: 'B-Object',\n",
    "    16: 'I-Object',\n",
    "    17: 'B-Materials',\n",
    "    18: 'I-Materials',\n",
    "    19: 'B-Result',\n",
    "    20: 'I-Result',\n",
    "    21: 'B-Quantity',\n",
    "    22: 'I-Quantity'\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '../models/distilbert-base-multilingual-cased-ner-hrl')\n",
    "\n",
    "def __expand_and_align_labels__(examples):\n",
    "        text = examples['text']\n",
    "        labels = []\n",
    "        data = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "        for event in examples['event_list']:\n",
    "            event_type = event['event_type']\n",
    "            trigger_text = event['trigger']['text']\n",
    "            trigger_offset = event['trigger']['offset']\n",
    "            arguments = [\n",
    "                (argument['role'], argument['text'], argument['offset'])\n",
    "                for argument in event['arguments']\n",
    "            ]\n",
    "            labels.append((event_type, trigger_text,\n",
    "                           trigger_offset, arguments))\n",
    "\n",
    "        # 将文本转换为标记的ID\n",
    "        input_tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        # 对齐标签到分词后的文本\n",
    "        for event_type, trigger_text, trigger_offset, arguments in labels:\n",
    "            # 对齐标签到分词后的文本\n",
    "            prompt_position = get_token_index(\n",
    "                text, input_tokens, trigger_offset[0])\n",
    "            prompt_text = f'{trigger_text}，类型{event_type}，位置{prompt_position}'\n",
    "            prompt_tokens = tokenizer.tokenize(prompt_text)\n",
    "\n",
    "            # 将Prompt和输入文本的分词结果合并\n",
    "            tokens = [tokenizer.cls_token] + prompt_tokens + \\\n",
    "                [tokenizer.sep_token] + input_tokens\n",
    "            aligned_labels = align_labels(text, arguments, input_tokens)\n",
    "            aligned_labels = ['O'] * \\\n",
    "                (len(tokens)-len(input_tokens))+aligned_labels\n",
    "            label_ids = [label_to_index[label] for label in aligned_labels]\n",
    "            inputs = tokenizer(tokens, is_split_into_words=True)\n",
    "            data['input_ids'].append(inputs['input_ids'])\n",
    "            data['attention_mask'].append(inputs['attention_mask'])\n",
    "            data['labels'].append(label_ids)\n",
    "        return {'data': data}\n",
    "    \n",
    "def to_flatten(examples):\n",
    "        inputs={'input_ids':[],'attention_masks':[],'labels':[]}\n",
    "        for ex_data in examples['data']:\n",
    "            for i in range(len(ex_data['input_ids'])):\n",
    "                inputs['input_ids'].append(ex_data['input_ids'][i])\n",
    "                inputs['attention_masks'].append(ex_data['attention_mask'][i])\n",
    "                inputs['labels'].append(ex_data['labels'][i])\n",
    "        return inputs\n",
    "\n",
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    #for sentence in examples[\"text\"]:\n",
    "    sentence=examples[\"text\"]\n",
    "    chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "def to_one(examples):\n",
    "    chunks2=[]\n",
    "    for chunks in examples[\"chunks\"]:\n",
    "        for sentence in chunks:\n",
    "            chunks2 += [sentence]\n",
    "    return {\"chunks2\": chunks2}\n",
    "        \n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= './data/FNDEE_train1.json')\n",
    "processed_dataset = dataset.map(\n",
    "        __expand_and_align_labels__, batched=False, remove_columns=[\"text\", \"event_list\", \"coref_arguments\", \"id\"])\n",
    "\n",
    "\n",
    "processed_dataset2=processed_dataset.map(to_flatten, batched=True, remove_columns=['data'])\n",
    "processed_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from event_dataset2 import EventDataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/distilbert-base-multilingual-cased-ner-hrl')\n",
    "event_dataset = EventDataset('./data/FNDEE_train1.json', './data/FNDEE_valid.json', tokenizer)\n",
    "len(event_dataset['train'])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    event_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [0],\n",
       " [1, 2],\n",
       " [3, 4],\n",
       " [5, 6],\n",
       " [7],\n",
       " [8],\n",
       " [9],\n",
       " [10],\n",
       " [11, 12],\n",
       " [13],\n",
       " [14, 15],\n",
       " [16],\n",
       " [17],\n",
       " [18],\n",
       " [19],\n",
       " [20],\n",
       " [21],\n",
       " [22],\n",
       " [23],\n",
       " [24, 25],\n",
       " [26],\n",
       " [27],\n",
       " [28, 29],\n",
       " [30],\n",
       " [31, 32],\n",
       " [33, 34],\n",
       " [35],\n",
       " [36],\n",
       " [37],\n",
       " [38],\n",
       " [39],\n",
       " [40, 42],\n",
       " [43],\n",
       " [44, 45],\n",
       " [46],\n",
       " [47, 48],\n",
       " [49],\n",
       " [50],\n",
       " [51, 52],\n",
       " [53, 54],\n",
       " [55],\n",
       " [56],\n",
       " [57],\n",
       " [58, 59],\n",
       " [60, 61],\n",
       " [62, 63],\n",
       " [64],\n",
       " [65, 67],\n",
       " [68, 69],\n",
       " [70],\n",
       " [71],\n",
       " [72, 74],\n",
       " [75, 76],\n",
       " [77],\n",
       " [78],\n",
       " [79],\n",
       " [80],\n",
       " [81],\n",
       " [82, 83],\n",
       " [84, 86],\n",
       " [87],\n",
       " [88],\n",
       " [89],\n",
       " [90],\n",
       " [91],\n",
       " [92],\n",
       " [93, 94],\n",
       " [95],\n",
       " [96],\n",
       " [97],\n",
       " [98, 99],\n",
       " [100, 101],\n",
       " [102, 103],\n",
       " [104, 105],\n",
       " [106],\n",
       " [107, 108],\n",
       " [109],\n",
       " [110],\n",
       " [111, 112],\n",
       " []]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/xlm-roberta-base')\n",
    "text=\"据美国国防官员1月3日透露，美国五角大楼将向海湾地区派出第二支航母打击群，以示对叙利亚和伊朗的警告，并赋予指挥官在该地区执行任务时更多的灵活性。国防部官员宣称，母港位于华盛顿州布雷默顿市约翰•C•斯坦尼斯航母打击群将于今月部署\"\n",
    "inputs = tokenizer(text,return_attention_mask=True,return_offsets_mapping=True)\n",
    "new_span=[]\n",
    "for i in inputs[\"offset_mapping\"]:\n",
    "        if i[0] == i[1]:\n",
    "                new_span.append([])\n",
    "        elif i[0] + 1 == i[1]:\n",
    "                new_span.append([i[0]])\n",
    "        else:\n",
    "                new_span.append([i[0], i[-1] - 1])\n",
    "tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
    "a=new_span\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
