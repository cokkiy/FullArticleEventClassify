{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"./data/test.json\")\n",
    "\n",
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns= ['text', 'id'])\n",
    "chunked_dataset['train'][:10]\n",
    "dataset\n",
    "chunked_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arguments_dataset2 import ArgumentsDataset\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from commonfn import align_labels, get_token_index\n",
    "\n",
    "label_to_index = {\n",
    "    # convert label to index\n",
    "    'O': 0,\n",
    "    'B-Subject': 1,\n",
    "    'I-Subject': 2,\n",
    "    'B-Equipment': 3,\n",
    "    'I-Equipment': 4,\n",
    "    'B-Date': 5,\n",
    "    'I-Date': 6,\n",
    "    'B-Location': 7,\n",
    "    'I-Location': 8,\n",
    "    'B-Area': 9,\n",
    "    'I-Area': 10,\n",
    "    'B-Content': 11,\n",
    "    'I-Content': 12,\n",
    "    'B-Militaryforce': 13,\n",
    "    'I-Militaryforce': 14,\n",
    "    'B-Object': 15,\n",
    "    'I-Object': 16,\n",
    "    'B-Materials': 17,\n",
    "    'I-Materials': 18,\n",
    "    'B-Result': 19,\n",
    "    'I-Result': 20,\n",
    "    'B-Quantity': 21,\n",
    "    'I-Quantity': 22\n",
    "}\n",
    "\n",
    "label_names = {\n",
    "    0: 'O',\n",
    "    1: 'B-Subject',\n",
    "    2: 'I-Subject',\n",
    "    3: 'B-Equipment',\n",
    "    4: 'I-Equipment',\n",
    "    5: 'B-Date',\n",
    "    6: 'I-Date',\n",
    "    7: 'B-Location',\n",
    "    8: 'I-Location',\n",
    "    9: 'B-Area',\n",
    "    10: 'I-Area',\n",
    "    11: 'B-Content',\n",
    "    12: 'I-Content',\n",
    "    13: 'B-Militaryforce',\n",
    "    14: 'I-Militaryforce',\n",
    "    15: 'B-Object',\n",
    "    16: 'I-Object',\n",
    "    17: 'B-Materials',\n",
    "    18: 'I-Materials',\n",
    "    19: 'B-Result',\n",
    "    20: 'I-Result',\n",
    "    21: 'B-Quantity',\n",
    "    22: 'I-Quantity'\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '../models/distilbert-base-multilingual-cased-ner-hrl')\n",
    "\n",
    "def __expand_and_align_labels__(examples):\n",
    "        text = examples['text']\n",
    "        labels = []\n",
    "        data = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "        for event in examples['event_list']:\n",
    "            event_type = event['event_type']\n",
    "            trigger_text = event['trigger']['text']\n",
    "            trigger_offset = event['trigger']['offset']\n",
    "            arguments = [\n",
    "                (argument['role'], argument['text'], argument['offset'])\n",
    "                for argument in event['arguments']\n",
    "            ]\n",
    "            labels.append((event_type, trigger_text,\n",
    "                           trigger_offset, arguments))\n",
    "\n",
    "        # 将文本转换为标记的ID\n",
    "        input_tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        # 对齐标签到分词后的文本\n",
    "        for event_type, trigger_text, trigger_offset, arguments in labels:\n",
    "            # 对齐标签到分词后的文本\n",
    "            prompt_position = get_token_index(\n",
    "                text, input_tokens, trigger_offset[0])\n",
    "            prompt_text = f'{trigger_text}，类型{event_type}，位置{prompt_position}'\n",
    "            prompt_tokens = tokenizer.tokenize(prompt_text)\n",
    "\n",
    "            # 将Prompt和输入文本的分词结果合并\n",
    "            tokens = [tokenizer.cls_token] + prompt_tokens + \\\n",
    "                [tokenizer.sep_token] + input_tokens\n",
    "            aligned_labels = align_labels(text, arguments, input_tokens)\n",
    "            aligned_labels = ['O'] * \\\n",
    "                (len(tokens)-len(input_tokens))+aligned_labels\n",
    "            label_ids = [label_to_index[label] for label in aligned_labels]\n",
    "            inputs = tokenizer(tokens, is_split_into_words=True)\n",
    "            data['input_ids'].append(inputs['input_ids'])\n",
    "            data['attention_mask'].append(inputs['attention_mask'])\n",
    "            data['labels'].append(label_ids)\n",
    "        return {'data': data}\n",
    "    \n",
    "def to_flatten(examples):\n",
    "        inputs={'input_ids':[],'attention_masks':[],'labels':[]}\n",
    "        for ex_data in examples['data']:\n",
    "            for i in range(len(ex_data['input_ids'])):\n",
    "                inputs['input_ids'].append(ex_data['input_ids'][i])\n",
    "                inputs['attention_masks'].append(ex_data['attention_mask'][i])\n",
    "                inputs['labels'].append(ex_data['labels'][i])\n",
    "        return inputs\n",
    "\n",
    "def chunk_examples(examples):\n",
    "    chunks = []\n",
    "    #for sentence in examples[\"text\"]:\n",
    "    sentence=examples[\"text\"]\n",
    "    chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "def to_one(examples):\n",
    "    chunks2=[]\n",
    "    for chunks in examples[\"chunks\"]:\n",
    "        for sentence in chunks:\n",
    "            chunks2 += [sentence]\n",
    "    return {\"chunks2\": chunks2}\n",
    "        \n",
    "\n",
    "dataset = load_dataset(\"json\", data_files= './data/FNDEE_train1.json')\n",
    "processed_dataset = dataset.map(\n",
    "        __expand_and_align_labels__, batched=False, remove_columns=[\"text\", \"event_list\", \"coref_arguments\", \"id\"])\n",
    "\n",
    "\n",
    "processed_dataset2=processed_dataset.map(to_flatten, batched=True, remove_columns=['data'])\n",
    "processed_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoConfig,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from event_dataset2 import EventDataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/distilbert-base-multilingual-cased-ner-hrl')\n",
    "event_dataset = EventDataset('./data/FNDEE_train1.json', './data/FNDEE_valid.json', tokenizer)\n",
    "len(event_dataset['train'])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    event_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 2, 3]), tensor([7, 8, 9])] [tensor([4, 5, 6]), tensor([10, 11, 12])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch=[{'a':[1,2,3],'b':[4,5,6]},{'a':[7,8,9],'b':[10,11,12]}]\n",
    "a_values = [torch.tensor(item['a']) for item in batch]\n",
    "b_values = [torch.tensor(item['b']) for item in batch]\n",
    "print(a_values,b_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
