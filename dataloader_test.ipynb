{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test EventDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  event_dataset import *\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# 加载预训练的GPT模型和分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/xlm-roberta-base')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"../models/xlm-roberta-base\")\n",
    "\n",
    "dataset=EventDataset('./data//dstest.json',tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for batch in dataloader:\n",
    "    input_ids, label_ids, texts, aligned_labels = batch\n",
    "    break\n",
    "    # 在这里执行模型训练的逻辑"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test for ArgumentsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0]), tensor([6]), tensor([84156]), tensor([4]), tensor([18617]), tensor([63652]), tensor([2]), tensor([6]), tensor([26432]), tensor([120716]), tensor([7051]), tensor([204716]), tensor([2649]), tensor([102702]), tensor([2575]), tensor([4]), tensor([26432]), tensor([264]), tensor([26449]), tensor([3990]), tensor([107870]), tensor([2767]), tensor([162920]), tensor([5730]), tensor([14830]), tensor([234090]), tensor([228503]), tensor([152636]), tensor([9243]), tensor([30]), tensor([6]), tensor([17236]), tensor([120716]), tensor([7051]), tensor([204716]), tensor([2649]), tensor([6959]), tensor([104168]), tensor([4]), tensor([4696]), tensor([2767]), tensor([162920]), tensor([5730]), tensor([14830]), tensor([234090]), tensor([32693]), tensor([84156]), tensor([3990]), tensor([9042]), tensor([6881]), tensor([4]), tensor([99212]), tensor([8856]), tensor([26449]), tensor([125047]), tensor([13052]), tensor([1726]), tensor([21848]), tensor([66374]), tensor([40513]), tensor([4]), tensor([69734]), tensor([418]), tensor([223455]), tensor([234090]), tensor([1726]), tensor([243494]), tensor([7818]), tensor([26449]), tensor([188644]), tensor([120716]), tensor([34421]), tensor([30]), tensor([6]), tensor([2767]), tensor([162920]), tensor([5730]), tensor([33531]), tensor([26432]), tensor([20424]), tensor([5260]), tensor([234090]), tensor([4]), tensor([2308]), tensor([76550]), tensor([470]), tensor([213]), tensor([6128]), tensor([109056]), tensor([5100]), tensor([12821]), tensor([15065]), tensor([42026]), tensor([4185]), tensor([5053]), tensor([93045]), tensor([4]), tensor([1273]), tensor([201488]), tensor([5039]), tensor([23381]), tensor([174874]), tensor([12370]), tensor([18635]), tensor([23737]), tensor([24754]), tensor([93045]), tensor([186200]), tensor([4]), tensor([213]), tensor([13730]), tensor([26449]), tensor([41309]), tensor([8522]), tensor([1826]), tensor([4395]), tensor([91720]), tensor([64995]), tensor([30]), tensor([16396]), tensor([470]), tensor([1662]), tensor([630]), tensor([4941]), tensor([234090]), tensor([213]), tensor([3589]), tensor([2767]), tensor([3327]), tensor([96605]), tensor([15036]), tensor([2003]), tensor([84156]), tensor([1898]), tensor([4]), tensor([12370]), tensor([244036]), tensor([3908]), tensor([75039]), tensor([135856]), tensor([47424]), tensor([12450]), tensor([1549]), tensor([487]), tensor([249930]), tensor([94007]), tensor([37]), tensor([3117]), tensor([487]), tensor([105811]), tensor([43]), tensor([28254]), tensor([30])] [tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([3]), tensor([3]), tensor([3]), tensor([3]), tensor([0]), tensor([0]), tensor([0]), tensor([4]), tensor([4]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0])] ('俄罗斯海军总司令部对外表示，俄罗斯和印度已经签署海豹号核潜艇验收交付书。 按照海军总司令部代表的意思，目前海豹号核潜艇所有的试验已经成功完成，新年之后印度艇员将登舰训练，明年1月下旬潜艇将驶往印度东部海军基地。 海豹号属于俄罗斯第三代潜艇，于1991年在阿穆尔造船厂开工建造，但上世纪90年代中期由于资金不足导致建造中止，在获得印度方面的投资后才得以恢复。2008年11月该潜艇在日本海进行工厂航行试验时，由于氟利昂泄漏造成20人牺牲、21人受伤的事故。 ',)\n"
     ]
    }
   ],
   "source": [
    "from arguments_dataset import ArgumentsDataset\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/xlm-roberta-base')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"../models/xlm-roberta-base\")\n",
    "dataset=ArgumentsDataset('./data//dstest.json',tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for batch in dataloader:\n",
    "    input_ids, label_ids, texts = batch\n",
    "    print(input_ids, label_ids, texts)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
